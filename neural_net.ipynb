{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c399f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from read_data import get_dirs\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d81add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhotoDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = Image.open(row[\"path\"]).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = torch.tensor(row[\"label\"], dtype=torch.long)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a3129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # change image size\n",
    "    transforms.Resize((224, 224)),\n",
    "    # converts PIL image to PyTorch tensor\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f2ebff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 224, 224]) torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "df = get_dirs()\n",
    "\n",
    "# initialising every image transformed\n",
    "dataset = PhotoDataset(df, transform=transform)\n",
    "\n",
    "# data wrapper for convenient usage to model\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# 16 images/labels, 3 colors, 224 height, 224 width\n",
    "# 16 at a time\n",
    "images, labels = next(iter(train_loader))\n",
    "print(images.shape, labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a0960c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cpu\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /Users/sumanth/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using:\", device)\n",
    "\n",
    "# resnet18 model with default weights (pre-trained IMAGENET1K_V1)\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# X*W + c, using final layer nodes to output ???\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)   # Alex vs Kelly\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee5964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer on weights using learning rate 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc98640e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Loss = 0.7322\n",
      "Batch 1: Loss = 0.6119\n",
      "Batch 2: Loss = 0.5328\n",
      "Batch 3: Loss = 0.6830\n",
      "Batch 4: Loss = 0.4426\n",
      "Batch 5: Loss = 0.5724\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for batch_idx, (imgs, lbls) in enumerate(train_loader):\n",
    "    imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "\n",
    "    # clear old gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # output of model\n",
    "    outputs = model(imgs)\n",
    "\n",
    "    # calculate loss based on current output/weights\n",
    "    loss = criterion(outputs, lbls)\n",
    "\n",
    "    # backward propagation to train model\n",
    "    loss.backward()\n",
    "\n",
    "    # forward propagation to update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Batch {batch_idx}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "    if batch_idx == 5:\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
