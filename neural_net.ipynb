{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c399f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from read_data import get_dirs\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eede900",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d81add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhotoDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = Image.open(row[\"path\"]).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = torch.tensor(row[\"label\"], dtype=torch.long)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6a3129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # change image size\n",
    "    transforms.Resize((224, 224)),\n",
    "    # converts PIL image to PyTorch tensor\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02f2ebff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 224, 224]) torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "df = get_dirs()\n",
    "\n",
    "# initialising every image transformed\n",
    "dataset = PhotoDataset(df, transform=transform)\n",
    "\n",
    "# data wrapper for convenient usage to model\n",
    "train_loader = DataLoader(dataset, batch_size=20, shuffle=True)\n",
    "\n",
    "# 20 images/labels, 3 colors, 224 height, 224 width\n",
    "# 20 at a time\n",
    "images, labels = next(iter(train_loader))\n",
    "print(images.shape, labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a31ee2",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93a0960c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using:\", device)\n",
    "\n",
    "# resnet18 model with default weights (pre-trained IMAGENET1K_V1)\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# X*W + c, using final layer nodes to output ???\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)   # Alex vs Kelly\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ee5964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer on weights using learning rate 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f57e84b",
   "metadata": {},
   "source": [
    "## Initial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc98640e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Loss = 0.0132, Acc = 1.000\n",
      "Batch 1: Loss = 0.0064, Acc = 1.000\n",
      "Batch 2: Loss = 0.0088, Acc = 1.000\n",
      "Batch 3: Loss = 0.0630, Acc = 0.950\n",
      "Batch 4: Loss = 0.1334, Acc = 0.950\n",
      "Batch 5: Loss = 0.2053, Acc = 0.950\n",
      "Batch 6: Loss = 0.0074, Acc = 1.000\n",
      "Batch 7: Loss = 0.0227, Acc = 1.000\n",
      "Batch 8: Loss = 0.0069, Acc = 1.000\n",
      "Batch 9: Loss = 0.0038, Acc = 1.000\n",
      "Batch 10: Loss = 0.0280, Acc = 1.000\n",
      "Batch 11: Loss = 0.0133, Acc = 1.000\n",
      "Batch 12: Loss = 0.0081, Acc = 1.000\n",
      "Batch 13: Loss = 0.0052, Acc = 1.000\n",
      "Batch 14: Loss = 0.0409, Acc = 1.000\n",
      "Batch 15: Loss = 0.0032, Acc = 1.000\n",
      "Batch 16: Loss = 0.0059, Acc = 1.000\n",
      "Batch 17: Loss = 0.0034, Acc = 1.000\n",
      "Batch 18: Loss = 0.0100, Acc = 1.000\n",
      "Batch 19: Loss = 0.1532, Acc = 1.000\n"
     ]
    }
   ],
   "source": [
    "batch_losses = []\n",
    "batch_accs = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "for batch_idx, (imgs, lbls) in enumerate(train_loader):\n",
    "    imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "\n",
    "    # clear old gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # output of model\n",
    "    outputs = model(imgs)\n",
    "\n",
    "    # calculate loss based on current output/weights\n",
    "    loss = criterion(outputs, lbls)\n",
    "\n",
    "    # backward propagation to train model\n",
    "    loss.backward()\n",
    "\n",
    "    # forward propagation to update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    preds = outputs.argmax(dim=1)\n",
    "    acc = (preds == lbls).float().mean().item()\n",
    "\n",
    "    batch_losses.append(loss.item())\n",
    "    batch_accs.append(acc)\n",
    "\n",
    "    print(f\"Batch {batch_idx}: Loss = {loss.item():.4f}, Acc = {acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07b7766",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bf4bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid: bs=20 lr=0.001 -> mean_acc=0.7464 (±0.0464)\n",
      "grid: bs=20 lr=0.0001 -> mean_acc=0.8577 (±0.0239)\n",
      "grid: bs=25 lr=0.001 -> mean_acc=0.7753 (±0.0640)\n",
      "grid: bs=25 lr=0.0001 -> mean_acc=0.8536 (±0.0272)\n",
      "grid: bs=26 lr=0.001 -> mean_acc=0.7608 (±0.0805)\n",
      "grid: bs=26 lr=0.0001 -> mean_acc=0.8536 (±0.0329)\n",
      "\n",
      "Best config: batch_size=20, lr=0.0001 -> mean_acc=0.8577 (±0.0239)\n",
      "Final train Epoch 1/5: loss=0.4837, acc=0.738\n",
      "Final train Epoch 2/5: loss=0.0791, acc=0.988\n",
      "Final train Epoch 3/5: loss=0.0341, acc=1.000\n",
      "Final train Epoch 4/5: loss=0.0167, acc=1.000\n",
      "Final train Epoch 5/5: loss=0.0144, acc=0.996\n",
      "Saved final_model_full.pth\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import itertools\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "batch_sizes = [20, 25, 26]\n",
    "lrs = [1e-3, 1e-4]\n",
    "num_folds = 5\n",
    "num_epochs_cv = 3\n",
    "num_epochs_final = 5\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "def cv_mean_acc(batch_size, lr):\n",
    "    fold_accs = []\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['label'])):\n",
    "        train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "        val_df   = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "        train_ds = PhotoDataset(train_df, transform=transform)\n",
    "        val_ds   = PhotoDataset(val_df,   transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "        val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "        model = model.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        best_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        for epoch in range(num_epochs_cv):\n",
    "            model.train()\n",
    "            for imgs, lbls in train_loader:\n",
    "                imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, lbls)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # validate\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for imgs, lbls in val_loader:\n",
    "                    imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "                    out = model(imgs)\n",
    "                    loss = criterion(out, lbls)\n",
    "                    val_loss += loss.item() * imgs.size(0)\n",
    "                    val_correct += (out.argmax(1) == lbls).sum().item()\n",
    "            epoch_val_loss = val_loss / len(val_ds)\n",
    "            if epoch_val_loss < best_val_loss:\n",
    "                best_val_loss = epoch_val_loss\n",
    "                best_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # final eval for fold using best_wts\n",
    "        model.load_state_dict(best_wts)\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, lbls in val_loader:\n",
    "                imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "                out = model(imgs)\n",
    "                val_correct += (out.argmax(1) == lbls).sum().item()\n",
    "        fold_accs.append(val_correct / len(val_ds))\n",
    "    return np.mean(fold_accs), np.std(fold_accs)\n",
    "\n",
    "# run grid search\n",
    "results = []\n",
    "for bs, lr in itertools.product(batch_sizes, lrs):\n",
    "    mean_acc, std_acc = cv_mean_acc(bs, lr)\n",
    "    results.append(((bs, lr), mean_acc, std_acc))\n",
    "    print(f\"grid: bs={bs} lr={lr} -> mean_acc={mean_acc:.4f} (±{std_acc:.4f})\")\n",
    "\n",
    "results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "best_cfg, best_mean, best_std = results[0]\n",
    "best_bs, best_lr = best_cfg\n",
    "print(f\"\\nBest config: batch_size={best_bs}, lr={best_lr} -> mean_acc={best_mean:.4f} (±{best_std:.4f})\")\n",
    "\n",
    "# retrain final model on full data with best hyperparams\n",
    "full_ds = PhotoDataset(df, transform=transform)\n",
    "full_loader = DataLoader(full_ds, batch_size=best_bs, shuffle=True)\n",
    "\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_lr)\n",
    "\n",
    "for epoch in range(num_epochs_final):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    for imgs, lbls in full_loader:\n",
    "        imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(imgs)\n",
    "        loss = criterion(out, lbls)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        running_correct += (out.argmax(1) == lbls).sum().item()\n",
    "    epoch_loss = running_loss / len(full_ds)\n",
    "    epoch_acc = running_correct / len(full_ds)\n",
    "    print(f\"Final train Epoch {epoch+1}/{num_epochs_final}: loss={epoch_loss:.4f}, acc={epoch_acc:.3f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"final_model_full.pth\")\n",
    "print(\"Saved final_model_full.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5c02e5",
   "metadata": {},
   "source": [
    "# Initial Predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
