{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c399f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from read_data import get_dirs\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eede900",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d81add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhotoDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = Image.open(row[\"path\"]).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = torch.tensor(row[\"label\"], dtype=torch.long)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6a3129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # change image size\n",
    "    transforms.Resize((224, 224)),\n",
    "    # converts PIL image to PyTorch tensor\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02f2ebff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 224, 224]) torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "df = get_dirs()\n",
    "\n",
    "# initialising every image transformed\n",
    "dataset = PhotoDataset(df, transform=transform)\n",
    "\n",
    "# data wrapper for convenient usage to model\n",
    "train_loader = DataLoader(dataset, batch_size=20, shuffle=True)\n",
    "\n",
    "# 20 images/labels, 3 colors, 224 height, 224 width\n",
    "# 20 at a time\n",
    "images, labels = next(iter(train_loader))\n",
    "print(images.shape, labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a31ee2",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93a0960c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using:\", device)\n",
    "\n",
    "# resnet18 model with default weights (pre-trained IMAGENET1K_V1)\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# X*W + c, using final layer nodes to output ???\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)   # Alex vs Kelly\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ee5964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer on weights using learning rate 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f57e84b",
   "metadata": {},
   "source": [
    "## Initial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc98640e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Loss = 0.8658, Acc = 0.500\n",
      "Batch 1: Loss = 0.7880, Acc = 0.600\n",
      "Batch 2: Loss = 0.7636, Acc = 0.550\n",
      "Batch 3: Loss = 0.5874, Acc = 0.650\n",
      "Batch 4: Loss = 0.6272, Acc = 0.650\n",
      "Batch 5: Loss = 0.5288, Acc = 0.750\n",
      "Batch 6: Loss = 0.4632, Acc = 0.750\n",
      "Batch 7: Loss = 0.7810, Acc = 0.400\n",
      "Batch 8: Loss = 0.3805, Acc = 0.800\n",
      "Batch 9: Loss = 0.6113, Acc = 0.750\n",
      "Batch 10: Loss = 0.4219, Acc = 0.750\n",
      "Batch 11: Loss = 0.5343, Acc = 0.750\n",
      "Batch 12: Loss = 0.4918, Acc = 0.750\n",
      "Batch 13: Loss = 0.3431, Acc = 0.850\n",
      "Batch 14: Loss = 0.4233, Acc = 0.800\n",
      "Batch 15: Loss = 0.5491, Acc = 0.800\n",
      "Batch 16: Loss = 0.2088, Acc = 0.950\n",
      "Batch 17: Loss = 0.4212, Acc = 0.750\n",
      "Batch 18: Loss = 0.3778, Acc = 0.800\n",
      "Batch 19: Loss = 0.5165, Acc = 0.800\n",
      "Batch 20: Loss = 0.2249, Acc = 0.900\n",
      "Batch 21: Loss = 0.3261, Acc = 0.850\n",
      "Batch 22: Loss = 0.2535, Acc = 0.900\n",
      "Batch 23: Loss = 0.4432, Acc = 0.800\n",
      "Batch 24: Loss = 0.5128, Acc = 0.800\n"
     ]
    }
   ],
   "source": [
    "batch_losses = []\n",
    "batch_accs = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "for batch_idx, (imgs, lbls) in enumerate(train_loader):\n",
    "    imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "\n",
    "    # clear old gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # output of model\n",
    "    outputs = model(imgs)\n",
    "\n",
    "    # calculate loss based on current output/weights\n",
    "    loss = criterion(outputs, lbls)\n",
    "\n",
    "    # backward propagation to train model\n",
    "    loss.backward()\n",
    "\n",
    "    # forward propagation to update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    preds = outputs.argmax(dim=1)\n",
    "    acc = (preds == lbls).float().mean().item()\n",
    "\n",
    "    batch_losses.append(loss.item())\n",
    "    batch_accs.append(acc)\n",
    "\n",
    "    print(f\"Batch {batch_idx}: Loss = {loss.item():.4f}, Acc = {acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c81bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39101c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e07b7766",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34bf4bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train Epoch 1/5: loss=0.5107, acc=0.761\n",
      "Final train Epoch 2/5: loss=0.0831, acc=0.984\n",
      "Final train Epoch 3/5: loss=0.0224, acc=1.000\n",
      "Final train Epoch 4/5: loss=0.0147, acc=1.000\n",
      "Final train Epoch 5/5: loss=0.0082, acc=1.000\n",
      "Saved final_model_full.pth\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "import itertools\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "# batch_sizes = [20, 25, 26]\n",
    "# lrs = [1e-3, 1e-4]\n",
    "# num_folds = 5\n",
    "# num_epochs_cv = 3\n",
    "# num_epochs_final = 5\n",
    "# skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# def cv_mean_acc(batch_size, lr):\n",
    "#     fold_accs = []\n",
    "#     for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['label'])):\n",
    "#         train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "#         val_df   = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "#         train_ds = PhotoDataset(train_df, transform=transform)\n",
    "#         val_ds   = PhotoDataset(val_df,   transform=transform)\n",
    "\n",
    "#         train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "#         val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#         model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "#         model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "#         model = model.to(device)\n",
    "\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#         best_val_loss = float('inf')\n",
    "#         best_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "#         for epoch in range(num_epochs_cv):\n",
    "#             model.train()\n",
    "#             for imgs, lbls in train_loader:\n",
    "#                 imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "#                 optimizer.zero_grad()\n",
    "#                 outputs = model(imgs)\n",
    "#                 loss = criterion(outputs, lbls)\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "\n",
    "#             # validate\n",
    "#             model.eval()\n",
    "#             val_correct = 0\n",
    "#             val_loss = 0.0\n",
    "#             with torch.no_grad():\n",
    "#                 for imgs, lbls in val_loader:\n",
    "#                     imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "#                     out = model(imgs)\n",
    "#                     loss = criterion(out, lbls)\n",
    "#                     val_loss += loss.item() * imgs.size(0)\n",
    "#                     val_correct += (out.argmax(1) == lbls).sum().item()\n",
    "#             epoch_val_loss = val_loss / len(val_ds)\n",
    "#             if epoch_val_loss < best_val_loss:\n",
    "#                 best_val_loss = epoch_val_loss\n",
    "#                 best_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "#         # final eval for fold using best_wts\n",
    "#         model.load_state_dict(best_wts)\n",
    "#         model.eval()\n",
    "#         val_correct = 0\n",
    "#         with torch.no_grad():\n",
    "#             for imgs, lbls in val_loader:\n",
    "#                 imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "#                 out = model(imgs)\n",
    "#                 val_correct += (out.argmax(1) == lbls).sum().item()\n",
    "#         fold_accs.append(val_correct / len(val_ds))\n",
    "#     return np.mean(fold_accs), np.std(fold_accs)\n",
    "\n",
    "# # run grid search\n",
    "# results = []\n",
    "# for bs, lr in itertools.product(batch_sizes, lrs):\n",
    "#     mean_acc, std_acc = cv_mean_acc(bs, lr)\n",
    "#     results.append(((bs, lr), mean_acc, std_acc))\n",
    "#     print(f\"grid: bs={bs} lr={lr} -> mean_acc={mean_acc:.4f} (±{std_acc:.4f})\")\n",
    "\n",
    "# results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "# best_cfg, best_mean, best_std = results[0]\n",
    "# best_bs, best_lr = best_cfg\n",
    "# print(f\"\\nBest config: batch_size={best_bs}, lr={best_lr} -> mean_acc={best_mean:.4f} (±{best_std:.4f})\")\n",
    "\n",
    "# retrain final model on full data with best hyperparams\n",
    "full_ds = PhotoDataset(df, transform=transform)\n",
    "full_loader = DataLoader(full_ds, batch_size=20, shuffle=True)\n",
    "\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    for imgs, lbls in full_loader:\n",
    "        imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(imgs)\n",
    "        loss = criterion(out, lbls)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        running_correct += (out.argmax(1) == lbls).sum().item()\n",
    "    epoch_loss = running_loss / len(full_ds)\n",
    "    epoch_acc = running_correct / len(full_ds)\n",
    "    print(f\"Final train Epoch {epoch+1}/{5}: loss={epoch_loss:.4f}, acc={epoch_acc:.3f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"final_model_full.pth\")\n",
    "print(\"Saved final_model_full.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a605715",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
